{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/fashion_mnist\"\n",
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "x_train,y_train = load_mnist(path)\n",
    "x_test,y_test = load_mnist(path,\"t10k\")\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs,num_outputs,num_hiddens = 784,10,256\n",
    "w1 = tf.random.normal([num_inputs,num_hiddens])\n",
    "b1 = tf.ones(shape=[num_hiddens])\n",
    "w2 = tf.random.normal(shape=[num_hiddens,num_outputs])\n",
    "b2 = tf.ones(shape=[num_outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.math.maximum(x,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,w1,b1,w2,b2):\n",
    "    out1 = relu( tf.matmul(x,w1) + b1 )\n",
    "    out2 = tf.math.softmax( tf.matmul(out1,w2) + b2 )\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=32, shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 4.7809088e-26,\n",
       "        0.0000000e+00, 1.0000000e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(x_train[0],dtype=tf.float32)\n",
    "x = tf.reshape(x,shape=[-1,num_inputs])\n",
    "model(x,w1,b1,w2,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data_inter(batch_size,features,lanbles):\n",
    "    num = len(features)\n",
    "    indices = list(range(num))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0,num,batch_size):\n",
    "        j = min(i+batch_size,num)\n",
    "        yield features[i:j],lanbles[i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat,y_true):\n",
    "    return tf.losses.sparse_categorical_crossentropy(y_true,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def acc(y_hat,y):\n",
    "    return np.mean((tf.argmax(y_hat,axis=1) == y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epcho: 1 loss: 3.272382\n",
      "epcho: 2 loss: 3.2749054\n",
      "epcho: 3 loss: 3.2748795\n",
      "epcho: 4 loss: 3.2749062\n",
      "epcho: 5 loss: 3.2750485\n",
      "epcho: 6 loss: 3.2753103\n",
      "epcho: 7 loss: 3.275365\n",
      "epcho: 8 loss: 3.2758248\n",
      "epcho: 9 loss: 3.2762804\n",
      "epcho: 10 loss: 3.2768219\n",
      "epcho: 11 loss: 3.277627\n",
      "epcho: 12 loss: 3.2781026\n",
      "epcho: 13 loss: 3.2785378\n",
      "epcho: 14 loss: 3.2791078\n",
      "epcho: 15 loss: 3.2794883\n",
      "epcho: 16 loss: 3.2798445\n",
      "epcho: 17 loss: 3.2800605\n",
      "epcho: 18 loss: 3.280244\n",
      "epcho: 19 loss: 3.28042\n",
      "epcho: 20 loss: 3.2806156\n",
      "epcho: 21 loss: 3.2808533\n",
      "epcho: 22 loss: 3.2814357\n",
      "epcho: 23 loss: 3.2817829\n",
      "epcho: 24 loss: 3.2820766\n",
      "epcho: 25 loss: 3.2823322\n",
      "epcho: 26 loss: 3.2846267\n",
      "epcho: 27 loss: 3.2849486\n",
      "epcho: 28 loss: 3.285148\n",
      "epcho: 29 loss: 3.2861626\n",
      "epcho: 30 loss: 3.2862995\n"
     ]
    }
   ],
   "source": [
    "num_epchos = 30\n",
    "lr = 0.001\n",
    "for i in range(num_epchos):\n",
    "    for x,y in data_inter(batch_size,x_train,y_train):\n",
    "        x = tf.constant(x,dtype=tf.float32) \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(w1)\n",
    "            tape.watch(b1)\n",
    "            tape.watch(w2)\n",
    "            tape.watch(b2)\n",
    "            y_hat = model(x,w1,b1,w2,b2)\n",
    "            l = loss(y_hat,y)\n",
    "        dw1,db1,dw2,db2 = tape.gradient(l,[w1,b1,w2,b2])\n",
    "        w1 -= lr * dw1/batch_size\n",
    "        w2 -= lr * dw2/batch_size\n",
    "        b1 -= lr * db1/batch_size\n",
    "        b2 -= lr * db2/batch_size\n",
    "    epcho_loss = loss(model(x,w1,b1,w2,b2),y)\n",
    "    print(\"epcho:\",i+1,\"loss:\",tf.reduce_mean(epcho_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7482421875"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_=[]\n",
    "for x,y in data_inter(batch_size,x_test,y_test):\n",
    "    x = tf.constant(x,dtype=tf.float32) \n",
    "    y_hat = model(x,w1,b1,w2,b2)\n",
    "    acc_.append(acc(y_hat,y))\n",
    "\n",
    "np.mean(acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
