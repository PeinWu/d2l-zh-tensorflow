{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据 (fashion-minit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/fashion_mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = load_mnist(path)\n",
    "x_test,y_test = load_mnist(path,\"t10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   1,   0,   0,  13,  73,   0,   0,   1,\n",
       "          4,   0,   0,   0,   0,   1,   1,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   3,   0,  36, 136, 127,  62,\n",
       "         54,   0,   0,   0,   1,   3,   4,   0,   0,   3,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0, 102, 204,\n",
       "        176, 134, 144, 123,  23,   0,   0,   0,   0,  12,  10,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,  72,\n",
       "         15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,  69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,\n",
       "         88, 172,  66,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          1,   1,   0, 200, 232, 232, 233, 229, 223, 223, 215, 213, 164,\n",
       "        127, 123, 196, 229,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0, 183, 225, 216, 223, 228, 235, 227, 224,\n",
       "        222, 224, 221, 223, 245, 173,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 193, 228, 218, 213, 198, 180,\n",
       "        212, 210, 211, 213, 223, 220, 243, 202,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   1,   3,   0,  12, 219, 220, 212, 218,\n",
       "        192, 169, 227, 208, 218, 224, 212, 226, 197, 209,  52,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99, 244, 222,\n",
       "        220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119, 167,  56,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0,   0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,\n",
       "          0, 237, 226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215,\n",
       "        218, 255,  77,   0,   0,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "         62, 145, 204, 228, 207, 213, 221, 218, 208, 211, 218, 224, 223,\n",
       "        219, 215, 224, 244, 159,   0,   0,   0,   0,   0,  18,  44,  82,\n",
       "        107, 189, 228, 220, 222, 217, 226, 200, 205, 211, 230, 224, 234,\n",
       "        176, 188, 250, 248, 233, 238, 215,   0,   0,  57, 187, 208, 224,\n",
       "        221, 224, 208, 204, 214, 208, 209, 200, 159, 245, 193, 206, 223,\n",
       "        255, 255, 221, 234, 221, 211, 220, 232, 246,   0,   3, 202, 228,\n",
       "        224, 221, 211, 211, 214, 205, 205, 205, 220, 240,  80, 150, 255,\n",
       "        229, 221, 188, 154, 191, 210, 204, 209, 222, 228, 225,   0,  98,\n",
       "        233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217, 241,\n",
       "         65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224, 229,\n",
       "         29,  75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206,\n",
       "        198, 213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220,\n",
       "        221, 230,  67,  48, 203, 183, 194, 213, 197, 185, 190, 194, 192,\n",
       "        202, 214, 219, 221, 220, 236, 225, 216, 199, 206, 186, 181, 177,\n",
       "        172, 181, 205, 206, 115,   0, 122, 219, 193, 179, 171, 183, 196,\n",
       "        204, 210, 213, 207, 211, 210, 200, 196, 194, 191, 195, 191, 198,\n",
       "        192, 176, 156, 167, 177, 210,  92,   0,   0,  74, 189, 212, 191,\n",
       "        175, 172, 175, 181, 185, 188, 189, 188, 193, 198, 204, 209, 210,\n",
       "        210, 211, 188, 188, 194, 192, 216, 170,   0,   2,   0,   0,   0,\n",
       "         66, 200, 222, 237, 239, 242, 246, 243, 244, 221, 220, 193, 191,\n",
       "        179, 182, 182, 181, 176, 166, 168,  99,  58,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0], dtype=uint8), 'ankle boot')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0],text_labels[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fun(x,y):\n",
    "    x = tf.cast(x,tf.float32)\n",
    "    return x / 255.0 ,tf.cast(y,tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformDs(ds):\n",
    "    return ds.shuffle(5000).batch(32).map(map_fun).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function map_fun at 0x00000152E3DDE1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <function map_fun at 0x00000152E3DDE1E0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "ds_train = transformDs(ds_train)\n",
    "ds_test = transformDs(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=7416, shape=(32, 784), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Tensor: id=7417, shape=(32,), dtype=int32, numpy=\n",
       " array([1, 0, 3, 4, 6, 9, 4, 6, 8, 9, 3, 0, 4, 8, 3, 2, 8, 3, 6, 4, 5, 7,\n",
       "        1, 8, 4, 6, 7, 3, 7, 5, 9, 6])>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32,activation=\"relu\"))#,kernel_initializer = tf.keras.initializers.he_normal()\n",
    "model.add(tf.keras.layers.Dense(10,activation=\"softmax\"))#,kernel_initializer = tf.keras.initializers.he_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,optimizer=tf.keras.optimizers.SGD(0.03),metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps, validate for 312 steps\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000152FBA16378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x00000152FBA16378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.6206 - acc: 0.7857 - val_loss: 0.5351 - val_acc: 0.8147\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4662 - acc: 0.8362 - val_loss: 0.4678 - val_acc: 0.8332\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4283 - acc: 0.8486 - val_loss: 0.4433 - val_acc: 0.8414\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4054 - acc: 0.8580 - val_loss: 0.4226 - val_acc: 0.8512\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3881 - acc: 0.8634 - val_loss: 0.4080 - val_acc: 0.8553\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3749 - acc: 0.8675 - val_loss: 0.4021 - val_acc: 0.8582\n",
      "Epoch 7/10\n",
      "  73/1875 [>.............................] - ETA: 4s - loss: 0.3627 - acc: 0.8682"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(x_train) // 32\n",
    "validation_steps = len(x_test) // 32\n",
    "model.fit(ds_train,epochs=10,steps_per_epoch=steps_per_epoch,validation_data = ds_test,validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
