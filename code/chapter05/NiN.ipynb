{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NiN使用1×1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去，下图对比了NiN同AlexNet和VGG等网络在结构上的主要区别\n",
    "\n",
    "![Image text](http://zh.d2l.ai/_images/nin.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(num_chanels,padding,strides,kernel_size):\n",
    "    net = tf.keras.Sequential()\n",
    "    net.add(tf.keras.layers.ZeroPadding2D(padding=padding))\n",
    "    net.add(tf.keras.layers.Conv2D(num_chanels,strides = strides,kernel_size = kernel_size,activation=\"relu\"))\n",
    "    net.add(tf.keras.layers.Conv2D(num_chanels,kernel_size=1,activation=\"relu\"))\n",
    "    net.add(tf.keras.layers.Conv2D(num_chanels,kernel_size=1,activation=\"relu\"))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=(224,224,3)))\n",
    "model.add(nin_block(96,kernel_size=11,strides=(4,4),padding=(0,0)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=3,strides=2))\n",
    "model.add(nin_block(256,kernel_size=5,padding=(2,2),strides=(1,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=3,strides=(2,2)))\n",
    "model.add(nin_block(384,kernel_size=3,strides=1,padding=(1,1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=3,strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(nin_block(10,kernel_size=3,strides=1,padding=(1,1)))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_14 (Sequential)   (None, 54, 54, 96)        53568     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 26, 26, 96)        0         \n",
      "_________________________________________________________________\n",
      "sequential_15 (Sequential)   (None, 26, 26, 256)       746240    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "sequential_16 (Sequential)   (None, 12, 12, 384)       1180800   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 384)         0         \n",
      "_________________________________________________________________\n",
      "sequential_17 (Sequential)   (None, 5, 5, 10)          34790     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,015,398\n",
      "Trainable params: 2,015,398\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 224, 224, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(224,224,3))\n",
    "x = tf.expand_dims(x,axis=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=9524, shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.09955324, 0.10052283, 0.09955355, 0.09955144, 0.10119517,\n",
       "        0.10093878, 0.09955144, 0.0996519 , 0.0997114 , 0.09977026]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"y:0\", shape=(5,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(fname=\"flower_photos\",origin=DATA_URL,untar=True)\n",
    "import pathlib\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "class_lable =  np.array([ item.name for item in data_dir.glob(\"*\") if item.name != \"LICENSE.txt\" ])\n",
    "## 转换lable为数值型数据\n",
    "lable_dic =dict( zip(class_lable,np.asarray(range(0,len(class_lable)))))\n",
    "@tf.function\n",
    "def get_lable(x):\n",
    "    arr_str = tf.strings.split(x,sep=\"\\\\\")\n",
    "    return arr_str[-2] == class_lable\n",
    "\n",
    "def decode_img(img):\n",
    "    img = tf.image.decode_image(img,channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize_with_crop_or_pad(img,IMG_WIDTH,IMG_HEIGHT)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img\n",
    "\n",
    "def img_map_fun(x):\n",
    "    lable = get_lable(x)\n",
    "    img = tf.io.read_file(x)\n",
    "    img = decode_img(img)\n",
    "    return img,lable\n",
    "\n",
    "lable = tf.constant([0,1,2,3,4])\n",
    "@tf.function\n",
    "def map_lable_fun(x,y):\n",
    "    print(y)\n",
    "    y = lable[y][0]\n",
    "    return x,y\n",
    "\n",
    "ds = ds.map(map_func=img_map_fun,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds = ds.map(map_func=map_lable_fun)\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000,batch_size = 256):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "    else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "ds = prepare_for_training(ds,batch_size = BATCH_SIZE)\n",
    "\n",
    "image_batch, label_batch = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10110, shape=(32, 10), dtype=float32, numpy=\n",
       "array([[0.09979424, 0.10017834, 0.09979573, 0.09978852, 0.10048943,\n",
       "        0.10053866, 0.09978912, 0.09985548, 0.09991376, 0.09985679],\n",
       "       [0.09982231, 0.10014329, 0.09982099, 0.09981655, 0.10037722,\n",
       "        0.10054319, 0.09981655, 0.09986108, 0.09994427, 0.09985459],\n",
       "       [0.09987631, 0.1000984 , 0.09986973, 0.09986672, 0.10026178,\n",
       "        0.10038055, 0.09986751, 0.09990907, 0.09998167, 0.09988821],\n",
       "       [0.09986027, 0.10012738, 0.09985249, 0.09985249, 0.10023209,\n",
       "        0.10041685, 0.09986018, 0.09992839, 0.09997012, 0.09989975],\n",
       "       [0.09978769, 0.10018132, 0.09979094, 0.09978241, 0.10048405,\n",
       "        0.10057083, 0.09978608, 0.09985456, 0.09992738, 0.09983474],\n",
       "       [0.09983917, 0.10015063, 0.09983138, 0.09982822, 0.10036184,\n",
       "        0.10044729, 0.09982993, 0.09988883, 0.0999561 , 0.09986661],\n",
       "       [0.09986003, 0.10010026, 0.0998689 , 0.09985211, 0.10031845,\n",
       "        0.10041818, 0.09985344, 0.09989356, 0.09995396, 0.099881  ],\n",
       "       [0.09983203, 0.10013421, 0.0998304 , 0.09981947, 0.10037036,\n",
       "        0.10052203, 0.09981947, 0.09986065, 0.09994402, 0.09986724],\n",
       "       [0.09983315, 0.10013397, 0.09982879, 0.09982599, 0.10034142,\n",
       "        0.10048899, 0.09983303, 0.09989092, 0.09996476, 0.09985896],\n",
       "       [0.09985105, 0.10014028, 0.09984931, 0.09984566, 0.10036151,\n",
       "        0.10037235, 0.09984566, 0.09989653, 0.09995592, 0.09988178],\n",
       "       [0.09983695, 0.10014124, 0.09984199, 0.09983421, 0.1003733 ,\n",
       "        0.10046449, 0.09983421, 0.09987107, 0.09992945, 0.09987308],\n",
       "       [0.099828  , 0.1001602 , 0.09982037, 0.09981766, 0.10041176,\n",
       "        0.10047756, 0.09981766, 0.09986153, 0.09994459, 0.09986069],\n",
       "       [0.09982086, 0.10015052, 0.09981994, 0.09981383, 0.10038266,\n",
       "        0.10050336, 0.09981596, 0.09987988, 0.09995486, 0.09985822],\n",
       "       [0.09980102, 0.10016989, 0.09981155, 0.09979549, 0.10049421,\n",
       "        0.10053327, 0.09979549, 0.09984154, 0.09991848, 0.09983906],\n",
       "       [0.09979697, 0.10017096, 0.09979745, 0.0997918 , 0.10045035,\n",
       "        0.1005476 , 0.09979751, 0.09987006, 0.09993978, 0.09983744],\n",
       "       [0.09984954, 0.10014243, 0.09985058, 0.0998466 , 0.10034083,\n",
       "        0.10040373, 0.0998466 , 0.09988952, 0.09994686, 0.09988333],\n",
       "       [0.09981979, 0.10013646, 0.09983306, 0.09981   , 0.10039075,\n",
       "        0.10055642, 0.09981001, 0.09984847, 0.09994944, 0.09984564],\n",
       "       [0.09978902, 0.10018231, 0.09978779, 0.09977814, 0.10047007,\n",
       "        0.10064786, 0.09977888, 0.09982857, 0.09991467, 0.09982272],\n",
       "       [0.0997987 , 0.1001702 , 0.09979593, 0.09979084, 0.10045007,\n",
       "        0.10058471, 0.09979084, 0.09985502, 0.09993288, 0.09983084],\n",
       "       [0.09985875, 0.10012735, 0.09984858, 0.09984563, 0.1002948 ,\n",
       "        0.1004243 , 0.0998481 , 0.09990817, 0.09996576, 0.09987859],\n",
       "       [0.09983771, 0.10013259, 0.09983032, 0.09982619, 0.10033228,\n",
       "        0.10049485, 0.0998308 , 0.09989122, 0.09996376, 0.09986021],\n",
       "       [0.09983294, 0.10015205, 0.09982695, 0.09982292, 0.10036555,\n",
       "        0.10047292, 0.09982464, 0.09988417, 0.09993841, 0.09987953],\n",
       "       [0.09989088, 0.10008332, 0.09989265, 0.09988743, 0.10021836,\n",
       "        0.10032995, 0.09988927, 0.09991897, 0.09997512, 0.09991413],\n",
       "       [0.09979269, 0.10015685, 0.09980661, 0.09978001, 0.10045939,\n",
       "        0.10065408, 0.09978001, 0.09982204, 0.09991308, 0.09983514],\n",
       "       [0.09981011, 0.1001651 , 0.09981257, 0.09980384, 0.10045397,\n",
       "        0.10050175, 0.0998047 , 0.09986533, 0.0999279 , 0.09985468],\n",
       "       [0.09986199, 0.10011785, 0.09985721, 0.09985324, 0.10030551,\n",
       "        0.10040255, 0.09985386, 0.09990058, 0.0999594 , 0.09988777],\n",
       "       [0.09985381, 0.10011636, 0.09985534, 0.09984706, 0.10033142,\n",
       "        0.10042261, 0.09984905, 0.09989053, 0.09995435, 0.09987936],\n",
       "       [0.09985235, 0.10012035, 0.099851  , 0.09984396, 0.10029055,\n",
       "        0.10045396, 0.09985019, 0.09989882, 0.09995693, 0.09988188],\n",
       "       [0.09985773, 0.10010909, 0.09986578, 0.09985063, 0.10031504,\n",
       "        0.1004318 , 0.09985063, 0.09988272, 0.09995813, 0.09987842],\n",
       "       [0.09990385, 0.10008826, 0.0999004 , 0.09989848, 0.10018443,\n",
       "        0.10030274, 0.09989913, 0.0999296 , 0.09996416, 0.09992903],\n",
       "       [0.0998393 , 0.10015461, 0.09983608, 0.09983287, 0.10039033,\n",
       "        0.10040731, 0.09983287, 0.09988576, 0.09993938, 0.09988153],\n",
       "       [0.09982824, 0.10012039, 0.0998356 , 0.09981927, 0.10035808,\n",
       "        0.10056611, 0.09981934, 0.099843  , 0.09994727, 0.09986264]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 50 steps\n",
      "Epoch 1/5\n",
      "50/50 [==============================] - 215s 4s/step - loss: 2.2721 - acc: 0.2163\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 233s 5s/step - loss: 2.2157 - acc: 0.2294\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 249s 5s/step - loss: 2.1759 - acc: 0.2381\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 242s 5s/step - loss: 2.2177 - acc: 0.2350\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 245s 5s/step - loss: 2.1466 - acc: 0.2587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4bc9f668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,metrics=[\"acc\"])\n",
    "model.fit(ds,steps_per_epoch=50,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
